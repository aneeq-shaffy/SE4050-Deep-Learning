{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJnOmpXFk6ngE4YJyy71ra",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneeq-shaffy/SE4050-Deep-Learning/blob/main/mbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orsmRv2GxeEF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3560b924"
      },
      "source": [
        "# Task\n",
        "Create a Google Colab notebook named \"mBERT\" that trains a multilingual BERT model for Sri Lankan news category classification using the dataset from \"https://github.com/nuuuwan/lk_news\". The notebook should include sections for setup, dataset loading and processing, label generation, text cleaning, data preparation for model training, and a placeholder for model training. The notebook should be fully runnable and include clear explanations for each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddbfdced"
      },
      "source": [
        "## Notebook setup\n",
        "\n",
        "### Subtask:\n",
        "Configure the notebook with a title, GPU runtime detection, library installations, optional Google Drive mounting, and necessary package imports.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14a31248"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires setting up the notebook environment, including checking for GPU, installing libraries, and importing packages. These steps can be grouped into a single code block for efficiency. Also, adding a title is a manual step in Colab settings and cannot be done via code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d7478e4",
        "outputId": "bd953012-d2e7-4fa9-cf14-8b38530e9474"
      },
      "source": [
        "# Check for GPU\n",
        "import tensorflow as tf\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "if gpu_available:\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    print(\"No GPU available.\")\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install transformers datasets pandas\n",
        "\n",
        "# Optional Google Drive mounting (uncomment if needed)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# print(\"Google Drive mounted.\")\n",
        "\n",
        "# Import required packages\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098e338e"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "### Subtask:\n",
        "Clone the dataset repository, explore the data structure, combine relevant files into a pandas DataFrame, and select required columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab2c0491"
      },
      "source": [
        "**Reasoning**:\n",
        "Clone the dataset repository and explore its structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eba19777",
        "outputId": "f4be7e5d-9651-4d15-8134-96d58a715c81"
      },
      "source": [
        "import os\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/nuuuwan/lk_news.git\n",
        "\n",
        "# Explore the directory structure\n",
        "!ls -R lk_news/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lk_news'...\n",
            "remote: Enumerating objects: 253620, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 253620 (delta 0), reused 1 (delta 0), pack-reused 253616 (from 4)\u001b[K\n",
            "Receiving objects: 100% (253620/253620), 876.17 MiB | 16.82 MiB/s, done.\n",
            "Resolving deltas: 100% (16166/16166), done.\n",
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64b4b434"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the JSON files into pandas DataFrames, concatenate them, and select the required columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25b4d0a1",
        "outputId": "f0ce015c-bad2-4109-d72b-41c9ff9e1589"
      },
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "all_files = glob.glob('lk_news/data/*/*/*.json')\n",
        "\n",
        "list_of_dfs = []\n",
        "\n",
        "for file_path in all_files:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            # Check if data is a list and not empty, then create DataFrame\n",
        "            if isinstance(data, list) and data:\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                list_of_dfs.append(df_temp)\n",
        "            # If data is a dictionary, convert to list and create DataFrame\n",
        "            elif isinstance(data, dict):\n",
        "                df_temp = pd.DataFrame([data])\n",
        "                list_of_dfs.append(df_temp)\n",
        "            else:\n",
        "                print(f\"Skipping file {file_path}: Data is not a list or dictionary or is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Skipping file {file_path}: JSON decode error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_path}: An error occurred - {e}\")\n",
        "\n",
        "\n",
        "if list_of_dfs:\n",
        "    df_combined = pd.concat(list_of_dfs, ignore_index=True)\n",
        "    df = df_combined[['url', 'text']]\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No dataframes were created. The combined dataframe is empty.\")\n",
        "    df = pd.DataFrame(columns=['url', 'text']) # Create an empty dataframe with required columns\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No dataframes were created. The combined dataframe is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea429282"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load data failed because the file path pattern was incorrect. Explore the directory structure again to find the correct path to the data files, then attempt to load and process the data again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de10fdae",
        "outputId": "631cfdbc-d655-4f74-8a45-b48919997a95"
      },
      "source": [
        "# Explore the directory structure again to find the correct data path\n",
        "!ls -R lk_news/\n",
        "\n",
        "# Based on the output, the data is likely in lk_news/data/\n",
        "# Try reading JSON files from lk_news/data/ directory\n",
        "\n",
        "all_files = glob.glob('lk_news/data/*/*/*/*.json')\n",
        "\n",
        "list_of_dfs = []\n",
        "\n",
        "for file_path in all_files:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            # Check if data is a list and not empty, then create DataFrame\n",
        "            if isinstance(data, list) and data:\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                list_of_dfs.append(df_temp)\n",
        "            # If data is a dictionary, convert to list and create DataFrame\n",
        "            elif isinstance(data, dict):\n",
        "                df_temp = pd.DataFrame([data])\n",
        "                list_of_dfs.append(df_temp)\n",
        "            else:\n",
        "                print(f\"Skipping file {file_path}: Data is not a list or dictionary or is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Skipping file {file_path}: JSON decode error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_path}: An error occurred - {e}\")\n",
        "\n",
        "\n",
        "if list_of_dfs:\n",
        "    df_combined = pd.concat(list_of_dfs, ignore_index=True)\n",
        "    df = df_combined[['url', 'text']]\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No dataframes were created. The combined dataframe is empty.\")\n",
        "    df = pd.DataFrame(columns=['url', 'text']) # Create an empty dataframe with required columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n",
            "No dataframes were created. The combined dataframe is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecd3f6eb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load data still failed, indicating the file path pattern is likely still incorrect. Explore the directory structure again, specifically within the `lk_news/` directory, to pinpoint where the actual data (JSON files) is located.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce9dfce8",
        "outputId": "549fde36-8fbf-49af-fe67-b1c52a503c68"
      },
      "source": [
        "# Explore the lk_news directory more thoroughly\n",
        "!ls -R lk_news/\n",
        "\n",
        "# Based on the output, it seems the data is not directly in a 'data' directory.\n",
        "# Let's check if the JSON files are within the 'src' directory or elsewhere.\n",
        "# We can use a more general glob pattern to find any JSON files within lk_news/\n",
        "all_files = glob.glob('lk_news/**/*.json', recursive=True)\n",
        "\n",
        "list_of_dfs = []\n",
        "\n",
        "for file_path in all_files:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            # Check if data is a list and not empty, then create DataFrame\n",
        "            if isinstance(data, list) and data:\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                list_of_dfs.append(df_temp)\n",
        "            # If data is a dictionary, convert to list and create DataFrame\n",
        "            elif isinstance(data, dict):\n",
        "                df_temp = pd.DataFrame([data])\n",
        "                list_of_dfs.append(df_temp)\n",
        "            else:\n",
        "                print(f\"Skipping file {file_path}: Data is not a list or dictionary or is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Skipping file {file_path}: JSON decode error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_path}: An error occurred - {e}\")\n",
        "\n",
        "\n",
        "if list_of_dfs:\n",
        "    df_combined = pd.concat(list_of_dfs, ignore_index=True)\n",
        "    # Check if 'url' and 'text' columns exist before selecting\n",
        "    required_columns = ['url', 'text']\n",
        "    if all(col in df_combined.columns for col in required_columns):\n",
        "        df = df_combined[required_columns]\n",
        "        display(df.head())\n",
        "    else:\n",
        "        missing = [col for col in required_columns if col not in df_combined.columns]\n",
        "        print(f\"Combined DataFrame is missing required columns: {missing}\")\n",
        "        df = pd.DataFrame(columns=required_columns) # Create an empty dataframe with required columns\n",
        "else:\n",
        "    print(\"No dataframes were created. The combined dataframe is empty.\")\n",
        "    df = pd.DataFrame(columns=['url', 'text']) # Create an empty dataframe with required columns\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n",
            "Combined DataFrame is missing required columns: ['url', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbd8f6aa"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to find data was unsuccessful. The `ls -R` output confirms that the `data` directory is not present directly in the cloned repository. It is likely that the data needs to be generated by running one of the scripts within the repository. The `workflows/pipeline.py` and `workflows/one_off_backpopulate.py` seem like potential candidates for data generation. Let's try running `workflows/one_off_backpopulate.py` as it might generate the data needed for the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b431479e",
        "outputId": "ef80c50d-ea4d-476e-8989-23c5fc61b43b"
      },
      "source": [
        "# Run the one_off_backpopulate.py script to potentially generate data\n",
        "!python lk_news/workflows/one_off_backpopulate.py\n",
        "\n",
        "# After running the script, check the directory structure again to see if a 'data' directory was created.\n",
        "!ls -R lk_news/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/lk_news/workflows/one_off_backpopulate.py\", line 4, in <module>\n",
            "    from utils import JSONFile, Log\n",
            "ModuleNotFoundError: No module named 'utils'\n",
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4661a917"
      },
      "source": [
        "**Reasoning**:\n",
        "The `one_off_backpopulate.py` script failed due to a `ModuleNotFoundError: No module named 'utils'`. This indicates that the required dependencies are not installed or the script is not being run in the correct environment. The repository has a `requirements.txt` file, which likely contains the necessary dependencies. Install the dependencies listed in `requirements.txt` and then try running the script again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638eda6d",
        "outputId": "733bcca2-b2a2-4a12-86cb-f0c171e2c95e"
      },
      "source": [
        "# Install dependencies from requirements.txt\n",
        "!pip install -r lk_news/requirements.txt\n",
        "\n",
        "# After installing dependencies, try running the one_off_backpopulate.py script again.\n",
        "!python lk_news/workflows/one_off_backpopulate.py\n",
        "\n",
        "# After running the script, check the directory structure again to see if a 'data' directory was created.\n",
        "!ls -R lk_news/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scraper-nuuuwan (from -r lk_news/requirements.txt (line 1))\n",
            "  Downloading scraper_nuuuwan-1.0.2-py3-none-any.whl.metadata (959 bytes)\n",
            "Collecting utils-nuuuwan (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_nuuuwan-2.0.11-py3-none-any.whl.metadata (725 bytes)\n",
            "Collecting bs4 (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting pymupdf (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.35.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.0.2)\n",
            "Collecting selenium (from scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.1.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.5.0)\n",
            "Collecting trio<1.0,>=0.30.0 (from selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2025.8.3)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.8.0)\n",
            "Collecting utils-ai-nuuuwan (from utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_ai_nuuuwan-1.0.5-py3-none-any.whl.metadata (617 bytes)\n",
            "Collecting utils-base-nuuuwan (from utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_base_nuuuwan-1.1.1-py3-none-any.whl.metadata (931 bytes)\n",
            "Collecting utils-git-nuuuwan (from utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_git_nuuuwan-2.0.1-py3-none-any.whl.metadata (535 bytes)\n",
            "Collecting utils-lang-nuuuwan (from utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_lang_nuuuwan-1.0.2-py3-none-any.whl.metadata (592 bytes)\n",
            "Collecting utils-twitter-nuuuwan (from utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_twitter_nuuuwan-1.0.1-py3-none-any.whl.metadata (570 bytes)\n",
            "Collecting utils-www-nuuuwan (from utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading utils_www_nuuuwan-1.3.4-py3-none-any.whl.metadata (832 bytes)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.12.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.4.0)\n",
            "Collecting outcome (from trio<1.0,>=0.30.0->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.7.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.109.1)\n",
            "Collecting mistralai (from utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting camelot-py (from utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.12.0.88)\n",
            "Collecting PyPDF2 (from utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.12/dist-packages (from utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.4.1)\n",
            "Collecting python-docx (from utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting deep-translator (from utils-lang-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting gTTS (from utils-lang-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (from utils-twitter-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.20.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (8.3.0)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (5.2.0)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.1.5)\n",
            "Collecting pdfminer-six>=20240706 (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdf<6.0,>=4.0 (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.12.0.88)\n",
            "Collecting pypdfium2>=4 (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.12/dist-packages (from geopy->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.1)\n",
            "Collecting click>=8.0.1 (from camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.28.1)\n",
            "Collecting invoke<3.0.0,>=2.2.0 (from mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1))\n",
            "  Downloading invoke-2.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.12/dist-packages (from mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.11.9)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (5.4.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy->utils-twitter-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (3.3.1)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy->utils-twitter-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (1.0.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.0->camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (43.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.10.3->mistralai->utils-ai-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py->utils-base-nuuuwan->utils-nuuuwan->scraper-nuuuwan->-r lk_news/requirements.txt (line 1)) (2.23)\n",
            "Downloading scraper_nuuuwan-1.0.2-py3-none-any.whl (22 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utils_nuuuwan-2.0.11-py3-none-any.whl (2.4 kB)\n",
            "Downloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading utils_ai_nuuuwan-1.0.5-py3-none-any.whl (7.8 kB)\n",
            "Downloading utils_base_nuuuwan-1.1.1-py3-none-any.whl (26 kB)\n",
            "Downloading utils_git_nuuuwan-2.0.1-py3-none-any.whl (3.7 kB)\n",
            "Downloading utils_lang_nuuuwan-1.0.2-py3-none-any.whl (4.0 kB)\n",
            "Downloading utils_twitter_nuuuwan-1.0.1-py3-none-any.whl (5.4 kB)\n",
            "Downloading utils_www_nuuuwan-1.3.4-py3-none-any.whl (4.0 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wsproto, python-docx, pypdfium2, PyPDF2, pypdf, pymupdf, outcome, invoke, eval-type-backport, click, trio, gTTS, deep-translator, bs4, trio-websocket, pdfminer-six, mistralai, selenium, camelot-py, utils-base-nuuuwan, utils-www-nuuuwan, utils-twitter-nuuuwan, utils-lang-nuuuwan, utils-git-nuuuwan, utils-ai-nuuuwan, utils-nuuuwan, scraper-nuuuwan\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.0\n",
            "    Uninstalling click-8.3.0:\n",
            "      Successfully uninstalled click-8.3.0\n",
            "Successfully installed PyPDF2-3.0.1 bs4-0.0.2 camelot-py-1.0.9 click-8.1.8 deep-translator-1.11.4 eval-type-backport-0.2.2 gTTS-2.5.4 invoke-2.2.0 mistralai-1.9.11 outcome-1.3.0.post0 pdfminer-six-20250506 pymupdf-1.26.4 pypdf-5.9.0 pypdfium2-4.30.0 python-docx-1.2.0 scraper-nuuuwan-1.0.2 selenium-4.36.0 trio-0.31.0 trio-websocket-0.12.2 utils-ai-nuuuwan-1.0.5 utils-base-nuuuwan-1.1.1 utils-git-nuuuwan-2.0.1 utils-lang-nuuuwan-1.0.2 utils-nuuuwan-2.0.11 utils-twitter-nuuuwan-1.0.1 utils-www-nuuuwan-1.3.4 wsproto-1.2.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lk_news/workflows/one_off_backpopulate.py\", line 6, in <module>\n",
            "    from lk_news import NewsArticle\n",
            "ModuleNotFoundError: No module named 'lk_news'\n",
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eadc4cd7"
      },
      "source": [
        "**Reasoning**:\n",
        "The `one_off_backpopulate.py` script still failed with a `ModuleNotFoundError: No module named 'lk_news'`. This suggests that the `lk_news` package, although present in the cloned repository's `src` directory, is not installed in the Python environment in a way that it can be imported directly. To fix this, we need to add the `src` directory to the Python path. After fixing the import issue, we can try running the script again, then attempt to load and process the generated data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "709df5df",
        "outputId": "d3cd58a6-d222-416b-89d0-16b56e9c3b2d"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Add the src directory to the Python path\n",
        "sys.path.append('./lk_news/src')\n",
        "\n",
        "# After adding to path, try running the one_off_backpopulate.py script again.\n",
        "# This script might take a while to run as it fetches data.\n",
        "print(\"Attempting to run the data generation script...\")\n",
        "!python lk_news/workflows/one_off_backpopulate.py\n",
        "print(\"Data generation script finished.\")\n",
        "\n",
        "# After running the script, check the directory structure again to see if a 'data' directory was created.\n",
        "print(\"Checking directory structure after script execution...\")\n",
        "!ls -R lk_news/\n",
        "print(\"Directory structure checked.\")\n",
        "\n",
        "# Now, try reading the JSON files again from the expected data directory.\n",
        "all_files = glob.glob('lk_news/data/*/*/*/*.json')\n",
        "\n",
        "list_of_dfs = []\n",
        "\n",
        "for file_path in all_files:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            # Check if data is a list and not empty, then create DataFrame\n",
        "            if isinstance(data, list) and data:\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                list_of_dfs.append(df_temp)\n",
        "            # If data is a dictionary, convert to list and create DataFrame\n",
        "            elif isinstance(data, dict):\n",
        "                df_temp = pd.DataFrame([data])\n",
        "                list_of_dfs.append(df_temp)\n",
        "            else:\n",
        "                # Some files might contain empty lists or non-dict/list data, skip those.\n",
        "                print(f\"Skipping file {file_path}: Data is not a list or dictionary or is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Skipping file {file_path}: JSON decode error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_path}: An error occurred - {e}\")\n",
        "\n",
        "\n",
        "if list_of_dfs:\n",
        "    df_combined = pd.concat(list_of_dfs, ignore_index=True)\n",
        "    # Check if 'url' and 'text' columns exist before selecting\n",
        "    required_columns = ['url', 'text']\n",
        "    if all(col in df_combined.columns for col in required_columns):\n",
        "        df = df_combined[required_columns]\n",
        "        print(\"\\nSuccessfully loaded and combined data.\")\n",
        "        display(df.head())\n",
        "    else:\n",
        "        missing = [col for col in required_columns if col not in df_combined.columns]\n",
        "        print(f\"\\nCombined DataFrame is missing required columns: {missing}\")\n",
        "        df = pd.DataFrame(columns=required_columns) # Create an empty dataframe with required columns\n",
        "else:\n",
        "    print(\"\\nNo dataframes were created. The combined dataframe is empty.\")\n",
        "    df = pd.DataFrame(columns=['url', 'text']) # Create an empty dataframe with required columns\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to run the data generation script...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lk_news/workflows/one_off_backpopulate.py\", line 6, in <module>\n",
            "    from lk_news import NewsArticle\n",
            "ModuleNotFoundError: No module named 'lk_news'\n",
            "Data generation script finished.\n",
            "Checking directory structure after script execution...\n",
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n",
            "Directory structure checked.\n",
            "\n",
            "No dataframes were created. The combined dataframe is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6cf135"
      },
      "source": [
        "**Reasoning**:\n",
        "The script still fails with `ModuleNotFoundError: No module named 'lk_news'`, even after adding `./lk_news/src` to the Python path. This suggests there might be an issue with how the package is structured or how the script is trying to import it. The `lk_news` package seems to be nested within the `src` directory. Let's try adding `./lk_news/src/` to the Python path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56c364ec",
        "outputId": "1118e353-e052-4aaf-dd38-6dedf8067497"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Add the lk_news package directory within src to the Python path\n",
        "sys.path.append('./lk_news/src/lk_news')\n",
        "\n",
        "# Try running the data generation script again.\n",
        "print(\"Attempting to run the data generation script again...\")\n",
        "!python lk_news/workflows/one_off_backpopulate.py\n",
        "print(\"Data generation script finished.\")\n",
        "\n",
        "# After running the script, check the directory structure again.\n",
        "print(\"Checking directory structure after script execution...\")\n",
        "!ls -R lk_news/\n",
        "print(\"Directory structure checked.\")\n",
        "\n",
        "# Now, try reading the JSON files again from the expected data directory.\n",
        "all_files = glob.glob('lk_news/data/*/*/*/*.json')\n",
        "\n",
        "list_of_dfs = []\n",
        "\n",
        "for file_path in all_files:\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            # Check if data is a list and not empty, then create DataFrame\n",
        "            if isinstance(data, list) and data:\n",
        "                df_temp = pd.DataFrame(data)\n",
        "                list_of_dfs.append(df_temp)\n",
        "            # If data is a dictionary, convert to list and create DataFrame\n",
        "            elif isinstance(data, dict):\n",
        "                df_temp = pd.DataFrame([data])\n",
        "                list_of_dfs.append(df_temp)\n",
        "            else:\n",
        "                # Some files might contain empty lists or non-dict/list data, skip those.\n",
        "                print(f\"Skipping file {file_path}: Data is not a list or dictionary or is empty.\")\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Skipping file {file_path}: JSON decode error.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {file_path}: An error occurred - {e}\")\n",
        "\n",
        "\n",
        "if list_of_dfs:\n",
        "    df_combined = pd.concat(list_of_dfs, ignore_index=True)\n",
        "    # Check if 'url' and 'text' columns exist before selecting\n",
        "    required_columns = ['url', 'text']\n",
        "    if all(col in df_combined.columns for col in required_columns):\n",
        "        df = df_combined[required_columns]\n",
        "        print(\"\\nSuccessfully loaded and combined data.\")\n",
        "        display(df.head())\n",
        "    else:\n",
        "        missing = [col for col in required_columns if col not in df_combined.columns]\n",
        "        print(f\"\\nCombined DataFrame is missing required columns: {missing}\")\n",
        "        df = pd.DataFrame(columns=required_columns) # Create an empty dataframe with required columns\n",
        "else:\n",
        "    print(\"\\nNo dataframes were created. The combined dataframe is empty.\")\n",
        "    df = pd.DataFrame(columns=['url', 'text']) # Create an empty dataframe with required columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to run the data generation script again...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/lk_news/workflows/one_off_backpopulate.py\", line 6, in <module>\n",
            "    from lk_news import NewsArticle\n",
            "ModuleNotFoundError: No module named 'lk_news'\n",
            "Data generation script finished.\n",
            "Checking directory structure after script execution...\n",
            "lk_news/:\n",
            "custom_summary.json  push_to_news_lk3.sh  requirements.txt  tests\n",
            "LICENSE\t\t     README.md\t\t  src\t\t    workflows\n",
            "\n",
            "lk_news/src:\n",
            "lk_news  news_lk3\n",
            "\n",
            "lk_news/src/lk_news:\n",
            "__init__.py  NewsArticle.py  NewspaperFactory.py\n",
            "\n",
            "lk_news/src/news_lk3:\n",
            "core  custom_newspapers  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core:\n",
            "AbstractNewsPaper.py  article  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/core/article:\n",
            "ArticleBase.py\tArticle.py  __init__.py\n",
            "\n",
            "lk_news/src/news_lk3/custom_newspapers:\n",
            "AdaDeranaLk.py\t\tDailyFtLk.py\t   __init__.py\n",
            "AdaDeranaSinhalaLk.py\tDailyMirrorLk.py   IslandLk.py\n",
            "AdaLk.py\t\tDailyNewsLk.py\t   LankadeepaLk.py\n",
            "BBCComSinhala.py\tDBSJeyarajCom.py   NewsFirstLk.py\n",
            "CeylonTodayLk.py\tDivainaLk.py\t   TamilMirrorLk.py\n",
            "ColomboTelegraphCom.py\tEconomyNextCom.py  VirakesariLk.py\n",
            "\n",
            "lk_news/tests:\n",
            "test_newspapers.py\n",
            "\n",
            "lk_news/workflows:\n",
            "one_off_backpopulate.py  pipeline.py\n",
            "Directory structure checked.\n",
            "\n",
            "No dataframes were created. The combined dataframe is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79a50611"
      },
      "source": [
        "## Label generation\n",
        "\n",
        "### Subtask:\n",
        "Infer a category label based on keywords in the 'url' column and display the category distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8d9d6a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to categorize URLs based on keywords and apply it to the 'url' column to create a new 'category' column. Then, display the distribution of the new 'category' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "ff05ea35",
        "outputId": "70b6183c-9e29-49b6-f801-b1113f1166a6"
      },
      "source": [
        "def categorize_url(url):\n",
        "    \"\"\"Categorizes a URL based on keywords.\"\"\"\n",
        "    if isinstance(url, str):\n",
        "        url_lower = url.lower()\n",
        "        if 'sport' in url_lower:\n",
        "            return 'sports'\n",
        "        elif 'business' in url_lower or 'biz' in url_lower:\n",
        "            return 'business'\n",
        "        elif 'politic' in url_lower: # Using 'politic' to catch variations like 'politics'\n",
        "            return 'politics'\n",
        "        elif 'entertain' in url_lower: # Using 'entertain' to catch variations\n",
        "            return 'entertainment'\n",
        "        elif 'news' in url_lower:\n",
        "            return 'news'\n",
        "        else:\n",
        "            return 'other'\n",
        "    else:\n",
        "        return 'other' # Handle potential non-string values\n",
        "\n",
        "# Apply the function to create the 'category' column\n",
        "df['category'] = df['url'].apply(categorize_url)\n",
        "\n",
        "# Display the distribution of the 'category' column\n",
        "display(df['category'].value_counts())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Series([], Name: count, dtype: int64)"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0cf6fe"
      },
      "source": [
        "## Text cleaning\n",
        "\n",
        "### Subtask:\n",
        "Clean the text data by removing URLs, non-letter symbols, and extra spaces, then filter out short articles and save the cleaned data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31fa803f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the text cleaning function, apply it to the 'text' column, filter the DataFrame based on text length, and display the head of the cleaned and filtered data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "561a71c4",
        "outputId": "d5f6de16-8986-4cc4-9b4b-966dd8744651"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans text data by removing URLs, non-letter symbols, and extra spaces.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # Remove non-letter symbols (keeping spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# Filter out short articles (e.g., less than 50 characters)\n",
        "min_text_length = 50\n",
        "df_cleaned_filtered = df[df['cleaned_text'].str.len() > min_text_length]\n",
        "\n",
        "# Display the head of the cleaned and filtered DataFrame\n",
        "display(df_cleaned_filtered.head())\n",
        "\n",
        "# Consider saving the cleaned DataFrame (optional)\n",
        "# df_cleaned_filtered.to_csv('cleaned_news_data.csv', index=False)\n",
        "# print(\"Cleaned and filtered data saved to 'cleaned_news_data.csv'\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [url, text, category, cleaned_text]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a28f5a9c-afb0-4512-ade2-51e8bcfb3e42\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>cleaned_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a28f5a9c-afb0-4512-ade2-51e8bcfb3e42')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a28f5a9c-afb0-4512-ade2-51e8bcfb3e42 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a28f5a9c-afb0-4512-ade2-51e8bcfb3e42');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76c8e31d"
      },
      "source": [
        "## Data preparation for training\n",
        "\n",
        "### Subtask:\n",
        "Split the cleaned data into training and testing sets, convert them to Hugging Face Datasets format, and tokenize the text data using a multilingual BERT tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0060b64e"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the cleaned data into training and testing sets, convert them to Hugging Face Datasets format, load the tokenizer, define the tokenization function, and apply it to the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df01d44c",
        "outputId": "e21f4e4c-a689-4210-c3d6-c76517ac4a48"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Split the cleaned and filtered data into training and testing sets\n",
        "# Check if df_cleaned_filtered is not empty before splitting\n",
        "if not df_cleaned_filtered.empty:\n",
        "    train_df, test_df = train_test_split(df_cleaned_filtered, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert pandas DataFrames to Hugging Face Dataset objects\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Load a multilingual BERT tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    # Define the tokenization function\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"cleaned_text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Apply the tokenization function to the datasets\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Display the first example of the tokenized training dataset\n",
        "    print(\"First example of the tokenized training dataset:\")\n",
        "    display(tokenized_train_dataset[0])\n",
        "\n",
        "else:\n",
        "    print(\"df_cleaned_filtered is empty. Cannot split data or tokenize.\")\n",
        "    # Create empty datasets to avoid errors in subsequent steps\n",
        "    train_dataset = Dataset.from_pandas(pd.DataFrame(columns=df_cleaned_filtered.columns))\n",
        "    test_dataset = Dataset.from_pandas(pd.DataFrame(columns=df_cleaned_filtered.columns))\n",
        "    tokenized_train_dataset = train_dataset\n",
        "    tokenized_test_dataset = test_dataset\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_cleaned_filtered is empty. Cannot split data or tokenize.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f31ba8f"
      },
      "source": [
        "## Model training (placeholder)\n",
        "\n",
        "### Subtask:\n",
        "This is a placeholder for the model training step. Due to the failure in previous steps to load and prepare the data, the actual model training cannot be performed at this time. This section outlines the steps that would be taken if the data were available.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae37d887"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps failed to load and prepare the data, resulting in empty dataframes. Therefore, the model training step cannot be performed as intended. I will outline the steps for model training as placeholders and finish the task with a failure status due to the lack of data.\n",
        "\n"
      ]
    }
  ]
}