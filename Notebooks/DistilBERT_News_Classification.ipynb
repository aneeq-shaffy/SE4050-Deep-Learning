{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "454d0b8a"
   },
   "source": [
    "# DistilBERT News Classification Model Development and Experimentation\n",
    "\n",
    "**Student ID:** IT20012892\n",
    "**Name:** Ahamed M.S.A\n",
    "\n",
    "This notebook details the process of developing and fine-tuning a news classification model using the DistilBERT transformer architecture. It includes steps for data loading, preprocessing, model training, evaluation, and experimentation with different hyperparameter configurations to optimize performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0e860c0"
   },
   "source": [
    "## Clone the Repository\n",
    "\n",
    "We will clone the repository containing the dataset and necessary scripts for this project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45dc966d",
    "outputId": "f66ada5c-7e21-4120-88a9-5c705d6186aa"
   },
   "source": [
    "!git clone https://github.com/aneeq-shaffy/SE4050-Deep-Learning.git"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'SE4050-Deep-Learning'...\n",
      "remote: Enumerating objects: 143, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
      "remote: Total 143 (delta 8), reused 17 (delta 4), pack-reused 116 (from 1)\u001b[K\n",
      "Receiving objects: 100% (143/143), 195.56 MiB | 18.82 MiB/s, done.\n",
      "Resolving deltas: 100% (44/44), done.\n",
      "Updating files: 100% (24/24), done.\n",
      "Encountered 8 file(s) that should have been pointers, but weren't:\n",
      "\tDataset/encoders/label_encoder.pkl\n",
      "\tDataset/processed/data_splits.pkl\n",
      "\tDataset/processed/news_preprocessed.csv\n",
      "\tDataset/reports/category_analysis.png\n",
      "\tDataset/reports/class_balancing.png\n",
      "\tDataset/reports/eda_comprehensive.png\n",
      "\tDataset/reports/quality_report.png\n",
      "\tDataset/reports/wordclouds_by_category.png\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "996fba07"
   },
   "source": [
    "## Setup and Library Installation\n",
    "\n",
    "Before proceeding with the model development, we need to install the necessary Python libraries, including `transformers`, `datasets`, `torch`, and `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets torch accelerate -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "064hJpfnnqsY",
    "outputId": "c498ee32-72da-4af9-f955-b8d570dd5d1d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e55d3db4"
   },
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "This section focuses on loading the preprocessed news dataset and the pre-split data for training, validation, and testing. We will examine the dataset's structure, the number of unique categories, and the distribution of labels."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load your data\n",
    "df = pd.read_csv('/content/SE4050-Deep-Learning/Dataset/processed/news_preprocessed.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Categories: {df['category'].nunique()}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Load pre-split data\n",
    "with open('/content/SE4050-Deep-Learning/Dataset/processed/data_splits.pkl', 'rb') as f:\n",
    "    data_splits = pickle.load(f)\n",
    "\n",
    "print(f\"\\nKeys in data_splits: {data_splits.keys()}\")\n",
    "\n",
    "train_texts = data_splits['X_train']\n",
    "val_texts = data_splits['X_val']\n",
    "test_texts = data_splits['X_test']\n",
    "train_labels = data_splits['y_train']\n",
    "val_labels = data_splits['y_val']\n",
    "test_labels = data_splits['y_test']\n",
    "\n",
    "print(f\"\\nTrain: {len(train_texts)}\")\n",
    "print(f\"Validation: {len(val_texts)}\")\n",
    "print(f\"Test: {len(test_texts)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zN-f9XASnx-Z",
    "outputId": "acda7047-e111-4400-ad4b-0fa84723041d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset shape: (30000, 3)\n",
      "Categories: 15\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0     2000\n",
      "1     2000\n",
      "2     2000\n",
      "3     2000\n",
      "4     2000\n",
      "5     2000\n",
      "6     2000\n",
      "7     2000\n",
      "8     2000\n",
      "9     2000\n",
      "10    2000\n",
      "11    2000\n",
      "12    2000\n",
      "13    2000\n",
      "14    2000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Keys in data_splits: dict_keys(['X_train', 'X_val', 'X_test', 'y_train', 'y_val', 'y_test'])\n",
      "\n",
      "Train: 21000\n",
      "Validation: 4500\n",
      "Test: 4500\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea441292"
   },
   "source": [
    "## Create Dataset Class\n",
    "\n",
    "We define a custom `Dataset` class to handle the data loading and tokenization process. This class prepares the data in a format suitable for training the DistilBERT model. It tokenizes the text, adds special tokens, pads and truncates sequences to a fixed length, and returns the input IDs, attention masks, and labels as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "id": "ABtas6BAoYmV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f072c2b0"
   },
   "source": [
    "## Initialize Tokenizer and Model\n",
    "\n",
    "This section initializes the DistilBERT tokenizer and loads the pre-trained DistilBERT model for sequence classification. We also create the training, validation, and test datasets using the custom `NewsDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "test_dataset = NewsDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
    "\n",
    "# Load model with number of labels\n",
    "num_labels = df['label'].nunique()\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"\\n\u2705 Model loaded with {num_labels} output classes\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376,
     "referenced_widgets": [
      "d7b94da65a954a86b4689bf0981741cd",
      "bac78bb5c329434e84683ab71a26b429",
      "cc5d432c28ee44b3ac22703abf85e5c9",
      "ff5f48c92c054461a0ab7a09da59c3a1",
      "90519bd01daf441f9abbf4210914cdb9",
      "149521b576b94c6fb97cfe5abe176255",
      "2111b3b628204458a341eb7a0bbd39a1",
      "6041561d198f4ad287935b6f0650393f",
      "d9892feaecd649cf97424ad229f6222d",
      "6d17a4b9a9484f8db85f015faf4ba9b6",
      "14aed8c6cfec40e58845ff6242c5cd84",
      "9ae0dd2dc5544c14afe7009f77213247",
      "27a57851772a4308883d407ac37ab7a7",
      "2255b6e69764441390377f52a4c71925",
      "27416d53d4f548a4ae92091d982bb6bc",
      "9b52502327634689961e5eefe0cf9703",
      "d077a91be94d4c469289c4285da69b38",
      "3ba62776e0564451b9ecd3a563230f8a",
      "d2183f9a70434f50ba1a57fd5221be44",
      "7243f6f6b31b4a66b66613066324ab5c",
      "c3d1b4e27f794c01824266a8c2b1e4a6",
      "7451bc36fd3a44869d5700fa3830e347",
      "17d5eeaeb79147f697233f0375d14027",
      "8535f7e2f4d14097935a6ede5614a15d",
      "64c00d273ca84fc881a8c41b0306d6cd",
      "acb1330ec07d493f8df1c12f60590dec",
      "9247fdd0048d4a258de37a2e1fef6399",
      "53e87a5a5ff540dca74bc997211e08b1",
      "2ec9f619f689414dafdac2950c42b071",
      "b587416d9d6f4be4817e341c41e4de5b",
      "767b420180684b68b77fbb1a5cda81f8",
      "8227479010d84943892e619730ce8426",
      "7a6e53b03f6e4721a1b45c53b4e7bc89",
      "37fff10e3bce48509d7357002d572f07",
      "74b1c4f10fd8449caf0b55227d459e90",
      "63dfc290430b4b96aecde968a87098f5",
      "6f6f21c83fa24865b22176c7563348e8",
      "b4033cb4ca9b45fabcbf96adaa59349d",
      "b1b019fc7e194023b93abcdaa1ef618d",
      "cff5cb4c3df0488091b8d314a05881b0",
      "6341b7eb41f44372aa8b8cfcd74c2942",
      "7a30ebcff86d4e519d403b61445b2358",
      "c58bae1a25a048658c67e62ac773bcca",
      "ffea16d79250418a991ba6afe7902253",
      "e2b89cb4d0e44a5096b830ef9f61a79b",
      "6e04e7cd153d4461a742bc46702b9fd8",
      "94c26e12bb9d4776b96f63a1452ada8d",
      "de2666cd74784cfb96414bf8de9799ac",
      "93bd7c6f44e0403d9b418eee6fe96099",
      "d717fab75c0c424fb1c57573b030d56b",
      "0035ea3f2b0348a697c84becc8eb9eb0",
      "e6e94b83f39e4d9ead3525d70d36a1a3",
      "575e2cc427544458beab91edc5822baa",
      "223e27b3993e4580879321ac74fa0aba",
      "8a74a950c363407caded5e5a6c9aa8eb"
     ]
    },
    "id": "b3HVr4EOoc5O",
    "outputId": "ecf68077-e554-46ae-a654-f547fc8c457b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7b94da65a954a86b4689bf0981741cd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ae0dd2dc5544c14afe7009f77213247"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17d5eeaeb79147f697233f0375d14027"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37fff10e3bce48509d7357002d572f07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2b89cb4d0e44a5096b830ef9f61a79b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Model loaded with 15 output classes\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60d37ea1"
   },
   "source": [
    "## Define Training Arguments\n",
    "\n",
    "This section defines the hyperparameters and configurations for the training process using the `TrainingArguments` class from the `transformers` library. These arguments control various aspects of training, such as the number of epochs, batch size, learning rate, and evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/SE4050-Deep-Learning/results/distilbert_news_model_results', # Updated output directory\n",
    "    num_train_epochs=3,              # Start with 3 epochs\n",
    "    per_device_train_batch_size=16,  # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',           # Evaluate during training\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    save_total_limit=2,              # Only keep 2 best checkpoints\n",
    "    report_to='none',                # Disable wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n",
    "\n",
    "print(\"Training configuration set \u2705\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxRmG17aoiYh",
    "outputId": "7b19292e-4715-4a32-b4ad-3c6ba7d5be38"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training configuration set \u2705\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5772d83"
   },
   "source": [
    "## Define Metrics\n",
    "\n",
    "We define a function `compute_metrics` to calculate evaluation metrics during training and evaluation. This function takes the model's predictions and the true labels and returns a dictionary containing relevant metrics like accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1\n",
    "    }"
   ],
   "metadata": {
    "id": "R4y4FTW2okAm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeaeb42d"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "This section initializes the `Trainer` with the model, training arguments, datasets, and metrics. Then, it starts the training process. We also include an early stopping callback to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\ud83d\ude80 Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "gt5jMH0Hor8I",
    "outputId": "c0e3432f-9f67-4573-f760-1579336ab386"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\ude80 Starting training...\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3939' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3939/3939 06:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.327200</td>\n",
       "      <td>1.244013</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.634554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.132900</td>\n",
       "      <td>1.096547</td>\n",
       "      <td>0.676667</td>\n",
       "      <td>0.671821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>1.086582</td>\n",
       "      <td>0.698444</td>\n",
       "      <td>0.698812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.820200</td>\n",
       "      <td>1.064947</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.692426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>0.979552</td>\n",
       "      <td>0.709778</td>\n",
       "      <td>0.708227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>1.035117</td>\n",
       "      <td>0.707778</td>\n",
       "      <td>0.708043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>1.046786</td>\n",
       "      <td>0.717333</td>\n",
       "      <td>0.715977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Training complete!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52d3b056"
   },
   "source": [
    "## Evaluate on Test Set\n",
    "\n",
    "After training, we evaluate the model's performance on the unseen test dataset to get an unbiased estimate of its generalization ability. This section calculates the accuracy, F1 score, and provides a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\n\ud83d\udcca Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Get detailed predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Detailed Classification Report:\")\n",
    "print(classification_report(test_labels, pred_labels))"
   ],
   "metadata": {
    "id": "WpPPvW67owhm",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "outputId": "08c0267e-f661-43b0-d9c9-a762f50ae76b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\ud83d\udcca Evaluating on test set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Test Results:\n",
      "eval_loss: 1.0726\n",
      "eval_accuracy: 0.7133\n",
      "eval_f1: 0.7122\n",
      "eval_runtime: 5.8328\n",
      "eval_samples_per_second: 771.4960\n",
      "eval_steps_per_second: 24.1740\n",
      "epoch: 3.0000\n",
      "\n",
      "\ud83d\udccb Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.59      0.63       300\n",
      "           1       0.68      0.68      0.68       300\n",
      "           2       0.77      0.88      0.82       300\n",
      "           3       0.58      0.57      0.58       300\n",
      "           4       0.61      0.75      0.67       300\n",
      "           5       0.78      0.85      0.81       300\n",
      "           6       0.63      0.55      0.58       300\n",
      "           7       0.74      0.69      0.71       300\n",
      "           8       0.67      0.69      0.68       300\n",
      "           9       0.84      0.75      0.79       300\n",
      "          10       0.81      0.86      0.84       300\n",
      "          11       0.86      0.76      0.81       300\n",
      "          12       0.66      0.74      0.69       300\n",
      "          13       0.61      0.57      0.59       300\n",
      "          14       0.79      0.76      0.78       300\n",
      "\n",
      "    accuracy                           0.71      4500\n",
      "   macro avg       0.71      0.71      0.71      4500\n",
      "weighted avg       0.71      0.71      0.71      4500\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41e77997"
   },
   "source": [
    "## Save Model and Tokenizer\n",
    "\n",
    "This section saves the fine-tuned DistilBERT model, the tokenizer, and the label mapping to  the google drive. This allows you to load and use the trained model later without retraining it."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('/content/SE4050-Deep-Learning/models/distilbert_news_model')\n",
    "tokenizer.save_pretrained('/content/SE4050-Deep-Learning/models/distilbert_news_model')\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = df[['category', 'label']].drop_duplicates().sort_values('label')\n",
    "with open('/content/SE4050-Deep-Learning/models/distilbert_news_model/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"\\n\u2705 Model, tokenizer, and label encoder saved!\")"
   ],
   "metadata": {
    "id": "PAVnyIUbo0Ah",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "063c93e8-cf34-4c60-f91f-948895c521df"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Model, tokenizer, and label encoder saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a921941c"
   },
   "source": [
    "## Test Predictions\n",
    "\n",
    "This section defines a function to make predictions on new text inputs using the trained model. It demonstrates how to use the model and tokenizer to classify example news headlines and shows the predicted category and confidence score."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_text(text, model, tokenizer, label_mapping):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0][pred_label].item()\n",
    "\n",
    "    # Get category name\n",
    "    category = label_mapping[label_mapping['label'] == pred_label]['category'].values[0]\n",
    "\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(f\"Predicted: {category}\")\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\\n\")\n",
    "\n",
    "# Test examples\n",
    "predict_text(\"The Lakers won the championship game last night\", model, tokenizer, label_mapping)\n",
    "predict_text(\"New study shows benefits of meditation\", model, tokenizer, label_mapping)\n",
    "predict_text(\"Stock market reaches all-time high\", model, tokenizer, label_mapping)"
   ],
   "metadata": {
    "id": "c_Lb1kclo3iS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ada0b8db-5e42-4e8e-ca84-8f392218cd67"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text: The Lakers won the championship game last night...\n",
      "Predicted: SPORTS\n",
      "Confidence: 98.99%\n",
      "\n",
      "Text: New study shows benefits of meditation...\n",
      "Predicted: WELLNESS\n",
      "Confidence: 98.53%\n",
      "\n",
      "Text: Stock market reaches all-time high...\n",
      "Predicted: BUSINESS\n",
      "Confidence: 99.00%\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c743c260"
   },
   "source": [
    "## Load Model Later\n",
    "\n",
    "This section demonstrates how to load the previously saved model and tokenizer to make predictions without retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# To load your trained model later\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import pickle\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained('/content/SE4050-Deep-Learning/models/distilbert_news_model')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('/content/SE4050-Deep-Learning/models/distilbert_news_model')\n",
    "\n",
    "with open('/content/SE4050-Deep-Learning/models/distilbert_news_model/label_encoder.pkl', 'rb') as f:\n",
    "    label_mapping = pickle.load(f)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"\u2705 Model loaded and ready!\")"
   ],
   "metadata": {
    "id": "LhrpgoM1o8Vz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "087a3a38-2ee7-496c-da17-1463f071a49b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Model loaded and ready!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "358e8063"
   },
   "source": [
    "# Critical Analysis and Experimentation for Model Improvement\n",
    "\n",
    "This section presents a critical analysis of the initial model's performance and details experiments conducted to explore different approaches for improving accuracy. We will discuss the results of each experiment and their implications for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e9c74d4"
   },
   "source": [
    "Based on our initial training using the preprocessed dataset and the DistilBERT architecture with default hyperparameters (3 epochs, max length 128), we achieved an accuracy of approximately 71% on the test set. The detailed classification report provides a breakdown of precision, recall, and F1-score for each category, highlighting areas where the model performs well and where there is room for improvement.\n",
    "\n",
    "\ud83d\udccb Detailed Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.69      0.59      0.63       300\n",
    "           1       0.68      0.68      0.68       300\n",
    "           2       0.77      0.88      0.82       300\n",
    "           3       0.58      0.57      0.58       300\n",
    "           4       0.61      0.75      0.67       300\n",
    "           5       0.78      0.85      0.81       300\n",
    "           6       0.63      0.55      0.58       300\n",
    "           7       0.74      0.69      0.71       300\n",
    "           8       0.67      0.69      0.68       300\n",
    "           9       0.84      0.75      0.79       00\n",
    "          10       0.81      0.86      0.84       300\n",
    "          11       0.86      0.76      0.81       300\n",
    "          12       0.66      0.74      0.69       300\n",
    "          13       0.61      0.57      0.59       300\n",
    "          14       0.79      0.76      0.78       300\n",
    "\n",
    "    accuracy                           0.71      4500\n",
    "   macro avg       0.71      0.71      0.71      4500\n",
    "weighted avg       0.71      0.71      0.71      4500\n",
    "\n",
    "To explore methods for enhancing this performance, we will now conduct a series of experiments by systematically varying key hyperparameters and observing their impact on the model's accuracy and other metrics. The goal is to identify configurations that lead to better generalization and overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4067fd9d"
   },
   "source": [
    "### Increasing Training Epochs\n",
    "\n",
    "In this experiment, we investigate the effect of increasing the number of training epochs on the model's performance. Training for more epochs can potentially allow the model to learn more complex patterns in the data, but it also increases the risk of overfitting. We will compare the results to the baseline model to see if increasing epochs leads to improved accuracy and observe the training and validation loss curves for signs of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\ud83d\ude80 Training with 10 epochs...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/SE4050-Deep-Learning/results/exp2',\n",
    "    num_train_epochs=10,  # \ud83d\udd25 3\u219210\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content/SE4050-Deep-Learning/logs/exp2',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "exp2_acc = test_results['eval_accuracy']\n",
    "\n",
    "print(f\"\\n\u2705 Baseline: 71.00% | Exp2: {exp2_acc*100:.2f}% | Improvement: {(exp2_acc-0.71)*100:+.2f}%\")\n",
    "\n",
    "# Save\n",
    "model.save_pretrained('/content/SE4050-Deep-Learning/models/distilbert_exp2')\n",
    "tokenizer.save_pretrained('/content/SE4050-Deep-Learning/models/distilbert_exp2')\n",
    "print(\"\u2705 Model saved!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "joYlkn72h3gg",
    "outputId": "8743470f-e72c-4358-a044-ea662039cd31"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\ude80 Training with 10 epochs...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='13130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2000/13130 03:56 < 21:57, 8.45 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.354500</td>\n",
       "      <td>1.255449</td>\n",
       "      <td>0.687333</td>\n",
       "      <td>0.690141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>1.271836</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.686363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.204300</td>\n",
       "      <td>1.480404</td>\n",
       "      <td>0.680444</td>\n",
       "      <td>0.680658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>1.500910</td>\n",
       "      <td>0.682444</td>\n",
       "      <td>0.681492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 00:06]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Baseline: 71.00% | Exp2: 68.42% | Improvement: -2.58%\n",
      "\u2705 Model saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8410eb0"
   },
   "source": [
    "**Conclusion:** Increasing the number of training epochs from 3 to 10 resulted in a decrease in test accuracy (from 71.00% to 68.42%).\n",
    "\n",
    "**Observation:** This decline in performance, despite continued training, suggests that the model started overfitting the training data. Overfitting occurs when the model learns the training data too well, including its noise and specific patterns, which negatively impacts its ability to generalize to unseen data like the test set. The early stopping mechanism likely prevented more severe overfitting, but the accuracy still dropped before it could fully intervene given the chosen patience. This indicates that simply increasing epochs without other adjustments is not beneficial and highlights the importance of monitoring validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40cdf1c1"
   },
   "source": [
    "### Extending Max Length\n",
    "\n",
    "In this experiment, we investigate the effect of increasing the maximum sequence length used for tokenization from 128 to 256 tokens. This allows the model to consider more context from the input text, which could potentially improve performance for longer news articles. We will observe if this change leads to an increase in accuracy and consider the trade-offs in terms of computational resources."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: INCREASED MAX LENGTH\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nChanges from Baseline:\")\n",
    "print(\"  \u2705 max_length: 128 \u2192 256\")\n",
    "print(\"  \u26a0\ufe0f  Note: This will use more GPU memory\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Recreate datasets with longer max_length\n",
    "print(\"\ud83d\udce6 Creating datasets with max_length=256...\")\n",
    "\n",
    "train_dataset_256 = NewsDataset(train_texts, train_labels, tokenizer, max_length=256)  # \ud83d\udd25 256\n",
    "val_dataset_256 = NewsDataset(val_texts, val_labels, tokenizer, max_length=256)  # \ud83d\udd25 256\n",
    "test_dataset_256 = NewsDataset(test_texts, test_labels, tokenizer, max_length=256)  # \ud83d\udd25 256\n",
    "\n",
    "print(\"\u2705 Datasets created with max_length=256\")\n",
    "\n",
    "# Reload model (fresh start)\n",
    "print(\"\\n\ud83d\udd04 Loading fresh model...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "print(\"\u2705 Model loaded\")\n",
    "\n",
    "# Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/SE4050-Deep-Learning/results/distilbert_exp3_results',\n",
    "    num_train_epochs=3,  # Keep at 3 to isolate the effect of max_length change\n",
    "    per_device_train_batch_size=8,  # \ud83d\udd25 REDUCED: 16 \u2192 8 (longer sequences need more memory)\n",
    "    per_device_eval_batch_size=16,  # \ud83d\udd25 REDUCED: 32 \u2192 16\n",
    "    gradient_accumulation_steps=2,  # \ud83d\udd25 ADDED: Effective batch = 8*2=16\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content/SE4050-Deep-Learning/logs/exp3',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Training configuration set\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Max Length: 256 \ud83d\udd25\")\n",
    "print(f\"   Batch Size: {training_args.per_device_train_batch_size} (reduced due to longer sequences)\")\n",
    "print(f\"   Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print()\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_256,  # \ud83d\udd25 Using new dataset\n",
    "    eval_dataset=val_dataset_256,      # \ud83d\udd25 Using new dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\ud83d\ude80 Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")\n",
    "print(f\"   Duration: {duration}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "print(\"\\n\ud83d\udcca Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset_256)  # \ud83d\udd25 Using new dataset\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f} ({test_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"Test F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "# Detailed predictions\n",
    "predictions = trainer.predict(test_dataset_256)  # \ud83d\udd25 Using new dataset\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Classification Report:\")\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "# Comparison with Previous Experiments\n",
    "baseline_acc = 0.71  # Baseline (3 epochs, 128 length)\n",
    "exp3_acc = test_results['eval_accuracy']\n",
    "improvement = exp3_acc - baseline_acc\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline (128 max_length):     {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
    "print(f\"Experiment 3 (256 max_length): {exp3_acc:.4f} ({exp3_acc*100:.2f}%)\")\n",
    "print(f\"Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save Model\n",
    "save_path = '/content/SE4050-Deep-Learning/models/distilbert_exp3_maxlen256'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"\\n\u2705 Model saved to: {save_path}\")\n",
    "\n",
    "# Save results\n",
    "import pickle\n",
    "results_dict = {\n",
    "    'experiment': 'Experiment 3 - Increased Max Length',\n",
    "    'max_length': 256,\n",
    "    'epochs': 3,\n",
    "    'baseline_acc': baseline_acc,\n",
    "    'test_acc': exp3_acc,\n",
    "    'improvement': improvement,\n",
    "    'duration': str(duration),\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('/content/SE4050-Deep-Learning/results/exp3_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_dict, f)\n",
    "\n",
    "# Save config for later use\n",
    "config_dict = {\n",
    "    'max_length': 256,\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'num_labels': num_labels\n",
    "}\n",
    "\n",
    "with open('/content/SE4050-Deep-Learning/models/distilbert_exp3_maxlen256/config.pkl', 'wb') as f:\n",
    "    pickle.dump(config_dict, f)\n",
    "\n",
    "print(\"\u2705 Results and config saved!\")\n",
    "print(\"\\n\ud83c\udf89 EXPERIMENT 3 COMPLETE!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ISD0tWOkkjWI",
    "outputId": "320fdd43-0cf7-465d-d26d-66b343cbf4ab"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 3: INCREASED MAX LENGTH\n",
      "======================================================================\n",
      "\n",
      "Changes from Baseline:\n",
      "  \u2705 max_length: 128 \u2192 256\n",
      "  \u26a0\ufe0f  Note: This will use more GPU memory\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udce6 Creating datasets with max_length=256...\n",
      "\u2705 Datasets created with max_length=256\n",
      "\n",
      "\ud83d\udd04 Loading fresh model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Model loaded\n",
      "\n",
      "\u2705 Training configuration set\n",
      "   Epochs: 3\n",
      "   Max Length: 256 \ud83d\udd25\n",
      "   Batch Size: 8 (reduced due to longer sequences)\n",
      "   Gradient Accumulation: 2\n",
      "\n",
      "\ud83d\ude80 Starting training...\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3939' max='3939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3939/3939 11:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.345200</td>\n",
       "      <td>1.257646</td>\n",
       "      <td>0.639333</td>\n",
       "      <td>0.637395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.128300</td>\n",
       "      <td>1.093779</td>\n",
       "      <td>0.675111</td>\n",
       "      <td>0.669337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.774400</td>\n",
       "      <td>1.077256</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.685655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.803700</td>\n",
       "      <td>1.058437</td>\n",
       "      <td>0.689556</td>\n",
       "      <td>0.689797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>0.978592</td>\n",
       "      <td>0.712889</td>\n",
       "      <td>0.712438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.482200</td>\n",
       "      <td>1.032262</td>\n",
       "      <td>0.710222</td>\n",
       "      <td>0.710138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>1.023952</td>\n",
       "      <td>0.721111</td>\n",
       "      <td>0.718871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Training complete!\n",
      "   Duration: 0:11:19.563932\n",
      "\n",
      "\ud83d\udcca Evaluating on test set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "Test Accuracy: 0.7176 (71.76%)\n",
      "Test F1 Score: 0.7160\n",
      "\n",
      "\ud83d\udccb Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.61      0.66       300\n",
      "           1       0.66      0.68      0.67       300\n",
      "           2       0.77      0.90      0.83       300\n",
      "           3       0.59      0.61      0.60       300\n",
      "           4       0.67      0.69      0.68       300\n",
      "           5       0.77      0.87      0.82       300\n",
      "           6       0.66      0.52      0.58       300\n",
      "           7       0.71      0.68      0.70       300\n",
      "           8       0.69      0.68      0.69       300\n",
      "           9       0.85      0.77      0.81       300\n",
      "          10       0.83      0.84      0.83       300\n",
      "          11       0.82      0.79      0.81       300\n",
      "          12       0.64      0.71      0.68       300\n",
      "          13       0.59      0.60      0.59       300\n",
      "          14       0.79      0.82      0.80       300\n",
      "\n",
      "    accuracy                           0.72      4500\n",
      "   macro avg       0.72      0.72      0.72      4500\n",
      "weighted avg       0.72      0.72      0.72      4500\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "Baseline (128 max_length):     0.7100 (71.00%)\n",
      "Experiment 3 (256 max_length): 0.7176 (71.76%)\n",
      "Improvement: +0.0076 (+0.76%)\n",
      "======================================================================\n",
      "\n",
      "\u2705 Model saved to: /content/SE4050-Deep-Learning/models/distilbert_exp3_maxlen256\n",
      "\u2705 Results and config saved!\n",
      "\n",
      "\ud83c\udf89 EXPERIMENT 3 COMPLETE!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53104e2f"
   },
   "source": [
    "**Conclusion:** Increasing the maximum sequence length from 128 to 256 tokens resulted in a slight improvement in test accuracy, from 71.00% to 71.76%.\n",
    "\n",
    "**Observation:** This small increase in accuracy suggests that allowing the model to process more of the text content provides a minor benefit. While the improvement is not substantial, it indicates that some news articles might benefit from the extended context. The computational cost is higher with longer sequences, which is a trade-off to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdaedf7e"
   },
   "source": [
    "### Combined Improvements\n",
    "\n",
    "In this experiment, we combine the changes from the previous two experiments to see if their combined effect leads to a greater improvement in performance. We will increase both the number of training epochs and the maximum sequence length for tokenization. We will observe the results and compare them to the baseline and the individual experiments."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 4: COMBINED IMPROVEMENTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nChanges from Baseline:\")\n",
    "print(\"  \u2705 num_train_epochs: 3 \u2192 10\")\n",
    "print(\"  \u2705 max_length: 128 \u2192 256\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Recreate datasets with max_length=256\n",
    "train_dataset_256 = NewsDataset(train_texts, train_labels, tokenizer, max_length=256)\n",
    "val_dataset_256 = NewsDataset(val_texts, val_labels, tokenizer, max_length=256)\n",
    "test_dataset_256 = NewsDataset(test_texts, test_labels, tokenizer, max_length=256)\n",
    "\n",
    "# Reload model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/SE4050-Deep-Learning/results/exp4',\n",
    "    num_train_epochs=10,  # \ud83d\udd25 Increased\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content/SE4050-Deep-Learning/logs/exp4',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "print(\"\u2705 Config: 10 epochs, 256 max_length\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_256,\n",
    "    eval_dataset=val_dataset_256,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "print(\"\ud83d\ude80 Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n\u2705 Training complete! Duration: {duration}\")\n",
    "\n",
    "# Evaluate\n",
    "test_results = trainer.evaluate(test_dataset_256)\n",
    "exp4_acc = test_results['eval_accuracy']\n",
    "\n",
    "predictions = trainer.predict(test_dataset_256)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline (3 epochs, 128 len):  71.00%\")\n",
    "print(f\"Exp 4 (10 epochs, 256 len):    {exp4_acc*100:.2f}%\")\n",
    "print(f\"Total Improvement:             {(exp4_acc-0.71)*100:+.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Classification Report:\")\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "# Save\n",
    "model.save_pretrained('/content/SE4050-Deep-Learning/models/distilbert_exp4_best')\n",
    "tokenizer.save_pretrained('/content/SE4050-Deep-Learning/models/distilbert_exp4_best')\n",
    "\n",
    "print(\"\\n\u2705 Best model saved!\")\n",
    "print(\"\ud83c\udf89 ALL EXPERIMENTS COMPLETE!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gRR37gNCnc-R",
    "outputId": "3ee02c0c-bbb1-4a1d-c89c-689dbc42ffa7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 4: COMBINED IMPROVEMENTS\n",
      "======================================================================\n",
      "\n",
      "Changes from Baseline:\n",
      "  \u2705 num_train_epochs: 3 \u2192 10\n",
      "  \u2705 max_length: 128 \u2192 256\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Config: 10 epochs, 256 max_length\n",
      "\ud83d\ude80 Starting training...\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='13130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6000/13130 18:00 < 21:24, 5.55 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.348200</td>\n",
       "      <td>1.272840</td>\n",
       "      <td>0.634889</td>\n",
       "      <td>0.632345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.133400</td>\n",
       "      <td>1.130337</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.659629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>1.111236</td>\n",
       "      <td>0.681111</td>\n",
       "      <td>0.681597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.840600</td>\n",
       "      <td>1.078567</td>\n",
       "      <td>0.685111</td>\n",
       "      <td>0.685688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.837700</td>\n",
       "      <td>1.017084</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.704553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>1.090727</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.698651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>1.124084</td>\n",
       "      <td>0.703778</td>\n",
       "      <td>0.704689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>1.179147</td>\n",
       "      <td>0.699556</td>\n",
       "      <td>0.700653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>1.351599</td>\n",
       "      <td>0.699111</td>\n",
       "      <td>0.695303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>1.392532</td>\n",
       "      <td>0.699333</td>\n",
       "      <td>0.700869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>1.506267</td>\n",
       "      <td>0.701111</td>\n",
       "      <td>0.701391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>1.593917</td>\n",
       "      <td>0.700222</td>\n",
       "      <td>0.698749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Training complete! Duration: 0:18:03.683764\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "Baseline (3 epochs, 128 len):  71.00%\n",
      "Exp 4 (10 epochs, 256 len):    69.51%\n",
      "Total Improvement:             -1.49%\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udccb Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.57      0.62       300\n",
      "           1       0.69      0.65      0.67       300\n",
      "           2       0.75      0.89      0.81       300\n",
      "           3       0.49      0.69      0.58       300\n",
      "           4       0.60      0.72      0.66       300\n",
      "           5       0.82      0.78      0.80       300\n",
      "           6       0.63      0.48      0.54       300\n",
      "           7       0.71      0.70      0.70       300\n",
      "           8       0.73      0.55      0.63       300\n",
      "           9       0.85      0.74      0.79       300\n",
      "          10       0.78      0.86      0.82       300\n",
      "          11       0.88      0.73      0.80       300\n",
      "          12       0.64      0.69      0.66       300\n",
      "          13       0.54      0.63      0.58       300\n",
      "          14       0.81      0.75      0.78       300\n",
      "\n",
      "    accuracy                           0.70      4500\n",
      "   macro avg       0.71      0.70      0.70      4500\n",
      "weighted avg       0.71      0.70      0.70      4500\n",
      "\n",
      "\n",
      "\u2705 Best model saved!\n",
      "\ud83c\udf89 ALL EXPERIMENTS COMPLETE!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dd7126a"
   },
   "source": [
    "**Conclusion:** Combining an increased number of training epochs (10) with an extended maximum sequence length (256) resulted in a decrease in test accuracy (from 71.00% baseline to 69.51%).\n",
    "\n",
    "**Observation:** This result suggests that simply combining hyperparameters that showed some individual promise (extended max length) with those that led to overfitting (increased epochs) does not necessarily lead to improved performance. The negative impact of increased epochs (overfitting) appears to outweigh the minor benefit of extended sequence length in this combined configuration. This highlights the complex interplay between different hyperparameters and the need for careful tuning rather than simply combining individual \"improvements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05c2df6a"
   },
   "source": [
    "### Optimizing Learning Rate and Scheduler\n",
    "\n",
    "In this experiment, we investigate the effect of optimizing the learning rate and using a learning rate scheduler on the model's performance. The learning rate controls the step size during model training, and a scheduler adjusts it over time. Properly tuning these can lead to more stable training and potentially better convergence to an optimal solution. We will test a lower learning rate with a cosine decay scheduler and observe its impact on accuracy and training stability."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 5: OPTIMIZED LEARNING RATE + SCHEDULER\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nChanges from Baseline:\")\n",
    "print(\"  \u2705 learning_rate: 5e-5 \u2192 2e-5 (lower, more stable)\")\n",
    "print(\"  \u2705 warmup_ratio: 0 \u2192 0.1 (10% warmup)\")\n",
    "print(\"  \u2705 lr_scheduler_type: linear \u2192 cosine (better decay)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Reload model (fresh start)\n",
    "print(\"\ud83d\udd04 Loading fresh model...\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "print(\"\u2705 Model loaded\")\n",
    "\n",
    "# Training Configuration with Optimized LR + Scheduler\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/SE4050-Deep-Learning/results/distilbert_exp5_results',\n",
    "    num_train_epochs=5,  # Using 5 epochs to see the effect\n",
    "\n",
    "    # \ud83d\udd25 LEARNING RATE OPTIMIZATION\n",
    "    learning_rate=2e-5,  # \ud83d\udd25 Lower LR (default is 5e-5)\n",
    "\n",
    "    # \ud83d\udd25 WARMUP SCHEDULE\n",
    "    warmup_ratio=0.1,  # \ud83d\udd25 Warmup for first 10% of training\n",
    "\n",
    "    # \ud83d\udd25 LEARNING RATE SCHEDULER\n",
    "    lr_scheduler_type='cosine',  # \ud83d\udd25 Cosine decay (smoother than linear)\n",
    "\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/content/SE4050-Deep-Learning/logs/exp5',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    save_total_limit=2,\n",
    "    report_to='none',\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Training configuration set\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Learning Rate: {training_args.learning_rate} (was 5e-5)\")\n",
    "print(f\"   Warmup Ratio: {training_args.warmup_ratio}\")\n",
    "print(f\"   LR Scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"   Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "print()\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Using original 128 max_length\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\ud83d\ude80 Starting training with optimized LR + Scheduler...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")\n",
    "print(f\"   Duration: {duration}\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "print(\"\\n\ud83d\udcca Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f} ({test_results['eval_accuracy']*100:.2f}%)\")\n",
    "print(f\"Test F1 Score: {test_results['eval_f1']:.4f}\")\n",
    "\n",
    "# Detailed predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Classification Report:\")\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "\n",
    "# Comparison with Baseline\n",
    "baseline_acc = 0.71  # Baseline\n",
    "exp5_acc = test_results['eval_accuracy']\n",
    "improvement = exp5_acc - baseline_acc\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline (default LR, 3 epochs):           {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
    "print(f\"Experiment 5 (optimized LR+scheduler, 5ep): {exp5_acc:.4f} ({exp5_acc*100:.2f}%)\")\n",
    "print(f\"Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save Model\n",
    "save_path = '/content/SE4050-Deep-Learning/models/distilbert_exp5_lr_optimized'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"\\n\u2705 Model saved to: {save_path}\")\n",
    "\n",
    "# Save results\n",
    "import pickle\n",
    "results_dict = {\n",
    "    'experiment': 'Experiment 5 - Optimized LR + Scheduler',\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'lr_scheduler': 'cosine',\n",
    "    'epochs': 5,\n",
    "    'baseline_acc': baseline_acc,\n",
    "    'test_acc': exp5_acc,\n",
    "    'improvement': improvement,\n",
    "    'duration': str(duration),\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('/content/SE4050-Deep-Learning/results/exp5_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_dict, f)\n",
    "\n",
    "print(\"\u2705 Results saved!\")\n",
    "print(\"\\n\ud83c\udf89 EXPERIMENT 5 COMPLETE!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OdkTpvn-qeza",
    "outputId": "e9848ca9-ae07-451a-c2b0-4a3b05525ad0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 5: OPTIMIZED LEARNING RATE + SCHEDULER\n",
      "======================================================================\n",
      "\n",
      "Changes from Baseline:\n",
      "  \u2705 learning_rate: 5e-5 \u2192 2e-5 (lower, more stable)\n",
      "  \u2705 warmup_ratio: 0 \u2192 0.1 (10% warmup)\n",
      "  \u2705 lr_scheduler_type: linear \u2192 cosine (better decay)\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udd04 Loading fresh model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Model loaded\n",
      "\n",
      "\u2705 Training configuration set\n",
      "   Epochs: 5\n",
      "   Learning Rate: 2e-05 (was 5e-5)\n",
      "   Warmup Ratio: 0.1\n",
      "   LR Scheduler: SchedulerType.COSINE\n",
      "   Batch Size: 16\n",
      "\n",
      "\ud83d\ude80 Starting training with optimized LR + Scheduler...\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6565' max='6565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6565/6565 14:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.779400</td>\n",
       "      <td>1.574244</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.589230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.193600</td>\n",
       "      <td>1.152289</td>\n",
       "      <td>0.669111</td>\n",
       "      <td>0.665319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.904400</td>\n",
       "      <td>1.088142</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.687082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>1.077997</td>\n",
       "      <td>0.690889</td>\n",
       "      <td>0.690665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>1.001810</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.705944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>1.042058</td>\n",
       "      <td>0.697333</td>\n",
       "      <td>0.696829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.652600</td>\n",
       "      <td>1.041465</td>\n",
       "      <td>0.703778</td>\n",
       "      <td>0.701871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.505500</td>\n",
       "      <td>1.022705</td>\n",
       "      <td>0.706444</td>\n",
       "      <td>0.704482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>1.065315</td>\n",
       "      <td>0.703556</td>\n",
       "      <td>0.702040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>1.071067</td>\n",
       "      <td>0.708222</td>\n",
       "      <td>0.707392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>1.084026</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.706232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>1.086180</td>\n",
       "      <td>0.707111</td>\n",
       "      <td>0.706156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>1.087316</td>\n",
       "      <td>0.708444</td>\n",
       "      <td>0.707638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u2705 Training complete!\n",
      "   Duration: 0:14:33.908631\n",
      "\n",
      "\ud83d\udcca Evaluating on test set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "Test Accuracy: 0.7040 (70.40%)\n",
      "Test F1 Score: 0.7035\n",
      "\n",
      "\ud83d\udccb Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65       300\n",
      "           1       0.67      0.65      0.66       300\n",
      "           2       0.79      0.85      0.82       300\n",
      "           3       0.56      0.60      0.58       300\n",
      "           4       0.66      0.66      0.66       300\n",
      "           5       0.78      0.85      0.81       300\n",
      "           6       0.58      0.55      0.56       300\n",
      "           7       0.72      0.65      0.68       300\n",
      "           8       0.67      0.68      0.67       300\n",
      "           9       0.83      0.75      0.79       300\n",
      "          10       0.82      0.86      0.84       300\n",
      "          11       0.82      0.77      0.80       300\n",
      "          12       0.63      0.70      0.66       300\n",
      "          13       0.58      0.57      0.58       300\n",
      "          14       0.77      0.79      0.78       300\n",
      "\n",
      "    accuracy                           0.70      4500\n",
      "   macro avg       0.70      0.70      0.70      4500\n",
      "weighted avg       0.70      0.70      0.70      4500\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "Baseline (default LR, 3 epochs):           0.7100 (71.00%)\n",
      "Experiment 5 (optimized LR+scheduler, 5ep): 0.7040 (70.40%)\n",
      "Improvement: -0.0060 (-0.60%)\n",
      "======================================================================\n",
      "\n",
      "\u2705 Model saved to: /content/SE4050-Deep-Learning/models/distilbert_exp5_lr_optimized\n",
      "\u2705 Results saved!\n",
      "\n",
      "\ud83c\udf89 EXPERIMENT 5 COMPLETE!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02574855"
   },
   "source": [
    "**Conclusion:** Optimizing the learning rate and using a cosine decay scheduler with 5 epochs resulted in a slight decrease in test accuracy (from 71.00% baseline to 70.40%).\n",
    "\n",
    "**Observation:** This indicates that the chosen learning rate and scheduler configuration, while potentially leading to more stable training in some scenarios, did not improve performance in this specific case. It is possible that further fine-tuning of these parameters or using a different number of epochs with this configuration could yield better results. This experiment highlights that optimizing learning rate and scheduler is not a guaranteed way to improve accuracy and requires careful experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5015ffb2"
   },
   "source": [
    "\n",
    "### How Accuracy Could Be Improved\n",
    "\n",
    "Based on the analysis, here are potential strategies for improving the model's accuracy:\n",
    "\n",
    "*   **Hyperparameter Tuning:** Conduct more systematic hyperparameter tuning to find the optimal combination of learning rate, batch size, weight decay, and other training arguments. Different optimizers could also be explored.\n",
    "*   **Further Text Preprocessing:** Investigate additional text preprocessing techniques tailored to news data, such as stemming, lemmatization, or removing highly frequent but uninformative words specific to news categories.\n",
    "*   **Exploring Different Model Architectures:** Experiment with other transformer models like BERT, RoBERTa, or even different types of neural networks that might capture different features or have better inductive biases for this specific news classification task.\n",
    "*   **Ensemble Methods:** Combine the predictions from multiple models trained with different configurations or architectures to leverage their individual strengths and potentially improve overall performance and robustness.\n",
    "*   **Data Augmentation:** Implement data augmentation techniques, particularly for categories where the model performed poorly, to increase the size and diversity of the training data for those categories.\n",
    "*   **Handling Imbalanced Classes:** If a more detailed analysis of the dataset reveals any subtle class imbalance not fully addressed, consider techniques like oversampling minority classes, undersampling majority classes, or using weighted loss functions during training to give more importance to minority classes.\n",
    "\n",
    "### Possible Future Work\n",
    "\n",
    "Here are potential directions for future research and development:\n",
    "\n",
    "*   **Larger and More Diverse Dataset:** Training the model on a significantly larger and more diverse news dataset could expose it to a wider range of linguistic patterns and topics, potentially leading to improved generalization.\n",
    "*   **Real-time News Classification System:** Develop a system capable of classifying news in real-time, which would involve considerations for efficient processing, continuous learning, and handling of streaming data.\n",
    "*   **Explainable AI (XAI):** Research and implement methods to understand *why* the model makes specific classification decisions. This could involve techniques to visualize attention mechanisms or identify key words influencing predictions, which can aid in model debugging and building trust.\n",
    "*   **Deployment:** Plan and execute the deployment of the trained model as a web service or integrate it into a news analysis application for practical use.\n",
    "*   **Investigating Specific Category Performance:** Conduct a deeper dive into the categories where the model showed weaker performance to identify the specific challenges (e.g., ambiguous language, lack of representative examples) and explore targeted solutions.\n",
    "*   **Transfer Learning with Domain Adaptation:** If the goal is to classify news from a very specific domain (e.g., financial news), explore fine-tuning a model pre-trained on a large news corpus and then further adapting it to the target domain using domain adaptation techniques."
   ]
  }
 ]
}